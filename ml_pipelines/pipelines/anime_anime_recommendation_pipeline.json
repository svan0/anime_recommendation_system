{
  "pipelineSpec": {
    "components": {
      "comp-component": {
        "executorLabel": "exec-component",
        "inputDefinitions": {
          "artifacts": {
            "anime_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "test_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "train_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "val_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "anime_embedding_size": {
              "type": "INT"
            },
            "data_format": {
              "type": "STRING"
            },
            "early_stop_num_epochs": {
              "type": "INT"
            },
            "learning_rate": {
              "type": "DOUBLE"
            },
            "max_num_epochs": {
              "type": "INT"
            },
            "optimizer": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics_path": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-component-2": {
        "executorLabel": "exec-component-2",
        "inputDefinitions": {
          "artifacts": {
            "anime_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "test_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "train_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "val_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "anime_embedding_size": {
              "type": "INT"
            },
            "data_format": {
              "type": "STRING"
            },
            "early_stop_num_epochs": {
              "type": "INT"
            },
            "learning_rate": {
              "type": "DOUBLE"
            },
            "max_num_epochs": {
              "type": "INT"
            },
            "optimizer": {
              "type": "STRING"
            },
            "scoring_layer_size": {
              "type": "INT"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics_path": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-component-3": {
        "executorLabel": "exec-component-3",
        "inputDefinitions": {
          "artifacts": {
            "input_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "data_format": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-component-4": {
        "executorLabel": "exec-component-4",
        "inputDefinitions": {
          "artifacts": {
            "input_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "data_format": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-component-5": {
        "executorLabel": "exec-component-5",
        "inputDefinitions": {
          "artifacts": {
            "anime_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "test_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "train_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "val_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "anime_embedding_size": {
              "type": "INT"
            },
            "data_format": {
              "type": "STRING"
            },
            "early_stop_num_epochs": {
              "type": "INT"
            },
            "learning_rate": {
              "type": "DOUBLE"
            },
            "max_num_epochs": {
              "type": "INT"
            },
            "optimizer": {
              "type": "STRING"
            },
            "scoring_layer_size": {
              "type": "INT"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "metrics_path": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-component-6": {
        "executorLabel": "exec-component-6",
        "inputDefinitions": {
          "artifacts": {
            "input_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "model_path": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "data_format": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-condition-no-run-retrieval-2": {
        "dag": {
          "outputs": {
            "artifacts": {
              "component-5-metrics_path": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "metrics_path",
                    "producerSubtask": "component-5"
                  }
                ]
              },
              "get-model-training-details-3-output_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "output_metrics",
                    "producerSubtask": "get-model-training-details-3"
                  }
                ]
              }
            }
          },
          "tasks": {
            "component-5": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-component-5"
              },
              "dependentTasks": [
                "run-query-save-to-bq-table-and-gcs-and-vertexai-11",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-12",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-13"
              ],
              "inputs": {
                "artifacts": {
                  "anime_data_path": {
                    "componentInputArtifact": "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path"
                  },
                  "test_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-13"
                    }
                  },
                  "train_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-11"
                    }
                  },
                  "val_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-12"
                    }
                  }
                },
                "parameters": {
                  "anime_embedding_size": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "128"
                      }
                    }
                  },
                  "data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "early_stop_num_epochs": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "1"
                      }
                    }
                  },
                  "learning_rate": {
                    "runtimeValue": {
                      "constantValue": {
                        "doubleValue": 0.005
                      }
                    }
                  },
                  "max_num_epochs": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "5"
                      }
                    }
                  },
                  "optimizer": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "adam"
                      }
                    }
                  },
                  "scoring_layer_size": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "128"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "TRAIN: anime anime ranking"
              }
            },
            "component-6": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-component-6"
              },
              "dependentTasks": [
                "component-5",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-10"
              ],
              "inputs": {
                "artifacts": {
                  "input_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-10"
                    }
                  },
                  "model_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model_path",
                      "producerTask": "component-5"
                    }
                  }
                },
                "parameters": {
                  "data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: anime anime ranking"
              }
            },
            "gcs-to-bq-table-and-vertexai-3": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-gcs-to-bq-table-and-vertexai-3"
              },
              "dependentTasks": [
                "component-6"
              ],
              "inputs": {
                "artifacts": {
                  "gcs_input_data": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "component-6"
                    }
                  }
                },
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_infer"
                      }
                    }
                  },
                  "gcs_input_data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "gcs_input_data_schema": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "[[\"anime_id\", \"STRING\"], [\"retrieved_anime_id\", \"STRING\"], [\"score\", \"FLOAT\"]]"
                      }
                    }
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: anime anime ranking to BQ"
              }
            },
            "get-model-training-details-3": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-get-model-training-details-3"
              },
              "dependentTasks": [
                "component-5"
              ],
              "inputs": {
                "artifacts": {
                  "input_metrics": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "metrics_path",
                      "producerTask": "component-5"
                    }
                  },
                  "input_model": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model_path",
                      "producerTask": "component-5"
                    }
                  }
                },
                "parameters": {
                  "labels": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"anime_embedding_size\": 128, \"scoring_layer_size\": 128, \"learning_rate\": 0.005, \"optimizer\": \"adam\", \"max_num_epochs\": 5, \"early_stop_num_epochs\": 1}"
                      }
                    }
                  },
                  "model_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_model"
                      }
                    }
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "TRAIN: anime anime ranking display"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-10": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-10"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_cross_anime"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n        WITH \n        list_anime AS (\n            \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n        )\n        SELECT A.anime_id, B.anime_id AS retrieved_anime_id\n        FROM list_anime A\n        CROSS JOIN list_anime B\n    "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: anime cross anime"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-11": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-11"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_train"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    ),\n    easy_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 < B.animeB_rank AND A.animeB_rank + 600 > B.animeB_rank\n    ),\n    hard_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank < B.animeB_rank AND A.animeB_rank + 100 > B.animeB_rank\n    ),\n    easy_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 > B.animeB_rank AND A.animeB_rank + 600 < B.animeB_rank\n    ),\n    hard_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank > B.animeB_rank AND A.animeB_rank + 100 < B.animeB_rank\n    ),\n    all_pairs AS (\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_negative_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_negative_pairs\n    )\n    \n        SELECT *\n        FROM all_pairs \n        WHERE ABS(MOD(FARM_FINGERPRINT(CONCAT(anime_id, retrieved_anime_id_1)), 10)) BETWEEN 0 AND 7 AND ABS(MOD(FARM_FINGERPRINT(retrieved_anime_id_2), 10)) = 0\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: train anime anime ranking"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-12": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-12"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_val"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    ),\n    easy_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 < B.animeB_rank AND A.animeB_rank + 600 > B.animeB_rank\n    ),\n    hard_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank < B.animeB_rank AND A.animeB_rank + 100 > B.animeB_rank\n    ),\n    easy_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 > B.animeB_rank AND A.animeB_rank + 600 < B.animeB_rank\n    ),\n    hard_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank > B.animeB_rank AND A.animeB_rank + 100 < B.animeB_rank\n    ),\n    all_pairs AS (\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_negative_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_negative_pairs\n    )\n    \n        SELECT *\n        FROM all_pairs \n        WHERE ABS(MOD(FARM_FINGERPRINT(CONCAT(anime_id, retrieved_anime_id_1)), 10)) = 8 AND ABS(MOD(FARM_FINGERPRINT(retrieved_anime_id_2), 10)) = 0\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: validation anime anime ranking"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-13": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-13"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_test"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    ),\n    easy_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 < B.animeB_rank AND A.animeB_rank + 600 > B.animeB_rank\n    ),\n    hard_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank < B.animeB_rank AND A.animeB_rank + 100 > B.animeB_rank\n    ),\n    easy_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 > B.animeB_rank AND A.animeB_rank + 600 < B.animeB_rank\n    ),\n    hard_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank > B.animeB_rank AND A.animeB_rank + 100 < B.animeB_rank\n    ),\n    all_pairs AS (\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_negative_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_negative_pairs\n    )\n    \n        SELECT *\n        FROM all_pairs \n        WHERE ABS(MOD(FARM_FINGERPRINT(CONCAT(anime_id, retrieved_anime_id_1)), 10)) = 9 AND ABS(MOD(FARM_FINGERPRINT(retrieved_anime_id_2), 10)) = 0\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: test anime anime ranking"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-14": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-14"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_user_last_watched"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n        WITH \n        list_anime AS (\n            \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n        ),\n        filtered_user_anime_on_anime AS (\n            \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n        ),\n        user_anime AS (\n            \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n     AND score IS NOT NULL AND score > 5\n        )\n        SELECT user_id, anime_id FROM user_anime WHERE user_anime_order = 1\n    "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: user last watched"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-15": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-15"
              },
              "dependentTasks": [
                "gcs-to-bq-table-and-vertexai-3",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-14"
              ],
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_user_anime_ranked"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n        WITH \n        initial_recommendations AS (\n            SELECT A.user_id, B.retrieved_anime_id AS anime_id, B.score\n            FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}.anime_anime_user_last_watched` A\n            LEFT JOIN `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}.anime_anime_ranking_infer` B\n            ON A.anime_id = B.anime_id\n        )\n        \n        SELECT user_id, anime_id, MAX(score) AS score\n        FROM (\n            SELECT A.user_id,\n                   B.animeB AS anime_id,\n                   A.score,  \n                   ROW_NUMBER() OVER (PARTITION BY A.user_id, A.anime_id ORDER BY B.related_order ASC) AS new_related_order\n            FROM initial_recommendations A\n            LEFT JOIN `anime-rec-dev.processed_area.related_priority` B\n            ON A.anime_id = B.animeA\n            LEFT JOIN `anime-rec-dev.processed_area.user_anime` C\n            ON B.animeB = C.anime_id\n            WHERE C.status IS NULL OR C.status = 'plan_to_watch'\n        )\n        WHERE new_related_order = 1\n        GROUP BY user_id, anime_id\n    \n    "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: user anime ranked"
              }
            }
          }
        },
        "inputDefinitions": {
          "artifacts": {
            "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "pipelineparam--data_format": {
              "type": "STRING"
            },
            "pipelineparam--dataset_id": {
              "type": "STRING"
            },
            "pipelineparam--project_id": {
              "type": "STRING"
            },
            "pipelineparam--run_retrieval": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "component-5-metrics_path": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "get-model-training-details-3-output_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-condition-yes-run-retrieval-1": {
        "dag": {
          "outputs": {
            "artifacts": {
              "component-2-metrics_path": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "metrics_path",
                    "producerSubtask": "component-2"
                  }
                ]
              },
              "component-metrics_path": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "metrics_path",
                    "producerSubtask": "component"
                  }
                ]
              },
              "get-model-training-details-2-output_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "output_metrics",
                    "producerSubtask": "get-model-training-details-2"
                  }
                ]
              },
              "get-model-training-details-output_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "output_metrics",
                    "producerSubtask": "get-model-training-details"
                  }
                ]
              }
            }
          },
          "tasks": {
            "component": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-component"
              },
              "dependentTasks": [
                "run-query-save-to-bq-table-and-gcs-and-vertexai-2",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-3",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-4"
              ],
              "inputs": {
                "artifacts": {
                  "anime_data_path": {
                    "componentInputArtifact": "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path"
                  },
                  "test_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-4"
                    }
                  },
                  "train_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-2"
                    }
                  },
                  "val_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-3"
                    }
                  }
                },
                "parameters": {
                  "anime_embedding_size": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "128"
                      }
                    }
                  },
                  "data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "early_stop_num_epochs": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "1"
                      }
                    }
                  },
                  "learning_rate": {
                    "runtimeValue": {
                      "constantValue": {
                        "doubleValue": 0.005
                      }
                    }
                  },
                  "max_num_epochs": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "5"
                      }
                    }
                  },
                  "optimizer": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "adam"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "TRAIN: anime anime retrieval"
              }
            },
            "component-2": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-component-2"
              },
              "dependentTasks": [
                "run-query-save-to-bq-table-and-gcs-and-vertexai-5",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-6",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-7"
              ],
              "inputs": {
                "artifacts": {
                  "anime_data_path": {
                    "componentInputArtifact": "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path"
                  },
                  "test_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-7"
                    }
                  },
                  "train_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-5"
                    }
                  },
                  "val_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai-6"
                    }
                  }
                },
                "parameters": {
                  "anime_embedding_size": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "128"
                      }
                    }
                  },
                  "data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "early_stop_num_epochs": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "1"
                      }
                    }
                  },
                  "learning_rate": {
                    "runtimeValue": {
                      "constantValue": {
                        "doubleValue": 0.005
                      }
                    }
                  },
                  "max_num_epochs": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "5"
                      }
                    }
                  },
                  "optimizer": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "adam"
                      }
                    }
                  },
                  "scoring_layer_size": {
                    "runtimeValue": {
                      "constantValue": {
                        "intValue": "128"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "TRAIN: anime anime ranking"
              }
            },
            "component-3": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-component-3"
              },
              "dependentTasks": [
                "component"
              ],
              "inputs": {
                "artifacts": {
                  "input_data_path": {
                    "componentInputArtifact": "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path"
                  },
                  "model_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model_path",
                      "producerTask": "component"
                    }
                  }
                },
                "parameters": {
                  "data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: anime anime retrieval"
              }
            },
            "component-4": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-component-4"
              },
              "dependentTasks": [
                "component-2",
                "component-3"
              ],
              "inputs": {
                "artifacts": {
                  "input_data_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "component-3"
                    }
                  },
                  "model_path": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model_path",
                      "producerTask": "component-2"
                    }
                  }
                },
                "parameters": {
                  "data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: anime anime ranking"
              }
            },
            "gcs-to-bq-table-and-vertexai": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-gcs-to-bq-table-and-vertexai"
              },
              "dependentTasks": [
                "component-3"
              ],
              "inputs": {
                "artifacts": {
                  "gcs_input_data": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "component-3"
                    }
                  }
                },
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_retrieval_infer"
                      }
                    }
                  },
                  "gcs_input_data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "gcs_input_data_schema": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "[[\"anime_id\", \"STRING\"], [\"retrieved_anime_id\", \"STRING\"]]"
                      }
                    }
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: anime anime retrieval to BQ"
              }
            },
            "gcs-to-bq-table-and-vertexai-2": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-gcs-to-bq-table-and-vertexai-2"
              },
              "dependentTasks": [
                "component-4"
              ],
              "inputs": {
                "artifacts": {
                  "gcs_input_data": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "output_data_path",
                      "producerTask": "component-4"
                    }
                  }
                },
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_infer"
                      }
                    }
                  },
                  "gcs_input_data_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "gcs_input_data_schema": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "[[\"anime_id\", \"STRING\"], [\"retrieved_anime_id\", \"STRING\"], [\"score\", \"FLOAT\"]]"
                      }
                    }
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: anime anime ranking to BQ"
              }
            },
            "get-model-training-details": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-get-model-training-details"
              },
              "dependentTasks": [
                "component"
              ],
              "inputs": {
                "artifacts": {
                  "input_metrics": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "metrics_path",
                      "producerTask": "component"
                    }
                  },
                  "input_model": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model_path",
                      "producerTask": "component"
                    }
                  }
                },
                "parameters": {
                  "labels": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"anime_embedding_size\": 128, \"learning_rate\": 0.005, \"optimizer\": \"adam\", \"max_num_epochs\": 5, \"early_stop_num_epochs\": 1}"
                      }
                    }
                  },
                  "model_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_retrieval_model"
                      }
                    }
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "TRAIN: anime anime retrieval display"
              }
            },
            "get-model-training-details-2": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-get-model-training-details-2"
              },
              "dependentTasks": [
                "component-2"
              ],
              "inputs": {
                "artifacts": {
                  "input_metrics": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "metrics_path",
                      "producerTask": "component-2"
                    }
                  },
                  "input_model": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model_path",
                      "producerTask": "component-2"
                    }
                  }
                },
                "parameters": {
                  "labels": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"anime_embedding_size\": 128, \"scoring_layer_size\": 128, \"learning_rate\": 0.005, \"optimizer\": \"adam\", \"max_num_epochs\": 5, \"early_stop_num_epochs\": 1}"
                      }
                    }
                  },
                  "model_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_model"
                      }
                    }
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "TRAIN: anime anime ranking display"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-2": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-2"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_retrieval_train"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    )\n    \n        SELECT animeA AS anime_id, animeB AS retrieved_anime_id\n        FROM anime_anime_ordered \n        WHERE (animeA_rank <= 100 OR animeB_rank <= 100) \n        AND animeA < animeB \n        AND ABS(MOD(FARM_FINGERPRINT(CONCAT(animeA, animeB)), 10)) BETWEEN 0 AND 7\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: train anime anime retrieval"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-3": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-3"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_retrieval_val"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    )\n    \n        SELECT animeA AS anime_id, animeB AS retrieved_anime_id\n        FROM anime_anime_ordered \n        WHERE (animeA_rank <= 100 OR animeB_rank <= 100)\n        AND animeA < animeB \n        AND ABS(MOD(FARM_FINGERPRINT(CONCAT(animeA, animeB)), 10)) = 8\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: val anime anime retrieval"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-4": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-4"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_retrieval_test"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    )\n    \n        SELECT animeA AS anime_id, animeB AS retrieved_anime_id\n        FROM anime_anime_ordered \n        WHERE (animeA_rank <= 100 OR animeB_rank <= 100) \n        AND animeA < animeB \n        AND ABS(MOD(FARM_FINGERPRINT(CONCAT(animeA, animeB)), 10)) = 9\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: test anime anime retrieval"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-5": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-5"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_train"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    ),\n    easy_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 < B.animeB_rank AND A.animeB_rank + 600 > B.animeB_rank\n    ),\n    hard_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank < B.animeB_rank AND A.animeB_rank + 100 > B.animeB_rank\n    ),\n    easy_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 > B.animeB_rank AND A.animeB_rank + 600 < B.animeB_rank\n    ),\n    hard_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank > B.animeB_rank AND A.animeB_rank + 100 < B.animeB_rank\n    ),\n    all_pairs AS (\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_negative_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_negative_pairs\n    )\n    \n        SELECT *\n        FROM all_pairs \n        WHERE ABS(MOD(FARM_FINGERPRINT(CONCAT(anime_id, retrieved_anime_id_1)), 10)) BETWEEN 0 AND 7 AND ABS(MOD(FARM_FINGERPRINT(retrieved_anime_id_2), 10)) = 0\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: train anime anime ranking"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-6": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-6"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_val"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    ),\n    easy_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 < B.animeB_rank AND A.animeB_rank + 600 > B.animeB_rank\n    ),\n    hard_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank < B.animeB_rank AND A.animeB_rank + 100 > B.animeB_rank\n    ),\n    easy_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 > B.animeB_rank AND A.animeB_rank + 600 < B.animeB_rank\n    ),\n    hard_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank > B.animeB_rank AND A.animeB_rank + 100 < B.animeB_rank\n    ),\n    all_pairs AS (\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_negative_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_negative_pairs\n    )\n    \n        SELECT *\n        FROM all_pairs \n        WHERE ABS(MOD(FARM_FINGERPRINT(CONCAT(anime_id, retrieved_anime_id_1)), 10)) = 8 AND ABS(MOD(FARM_FINGERPRINT(retrieved_anime_id_2), 10)) = 0\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: validation anime anime ranking"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-7": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-7"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_ranking_test"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n    WITH \n    list_anime AS (\n        \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n    ),\n    anime_info AS(\n        SELECT A.* FROM `anime-rec-dev.processed_area.anime` A INNER JOIN list_anime B ON A.anime_id = B.anime_id\n    ),\n    filtered_user_anime_on_anime AS (\n        \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n    ),\n    list_users AS (\n        \n        SELECT user_id\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY user_id\n        HAVING COUNT(*) >= 50\n    \n    ),\n    filtered_user_anime AS (\n        \n        SELECT A.*\n        FROM filtered_user_anime_on_anime A\n        INNER JOIN list_users B\n        ON A.user_id = B.user_id\n    \n    ),\n    user_anime AS (\n        \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n    \n    ),\n    anime_co_completed_anime AS (\n        \n        SELECT animeA, animeB, co_occurence_cnt, NTILE(5) OVER (PARTITION BY animeA ORDER BY co_occurence_cnt ASC) AS bucket_co_occurence_cnt\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, COUNT(*) AS co_occurence_cnt\n            FROM user_anime A \n            LEFT JOIN user_anime B\n            ON A.user_id = B.user_id \n            WHERE (ABS(A.user_anime_order - B.user_anime_order) BETWEEN 1 AND 10)\n            GROUP BY A.anime_id, B.anime_id\n        )\n    \n    ),\n    anime_related_anime AS (\n        \n        SELECT animeA, animeB, related\n        FROM `anime-rec-dev.processed_area.anime_anime`\n        WHERE related = 1\n    \n    ),\n    anime_recommended_anime AS (\n        \n        SELECT animeA, animeB, num_recommenders, NTILE(5) OVER (PARTITION BY animeA ORDER BY num_recommenders ASC) AS bucket_num_recommenders\n        FROM `anime-rec-dev.processed_area.anime_anime` \n        WHERE recommendation = 1\n    \n    ),\n    anime_genre_sim_anime AS (\n        \n        SELECT animeA, animeB, cosine_sim, NTILE(5) OVER (PARTITION BY animeA ORDER BY cosine_sim ASC) AS bucket_cosine_sim\n        FROM (\n            SELECT A.anime_id AS animeA, B.anime_id AS animeB, ARRAY_LENGTH(functions.array_intersect(A.genres, B.genres)) / (POW(ARRAY_LENGTH(A.genres), 2) + POW(ARRAY_LENGTH(B.genres), 2)) AS cosine_sim\n            FROM anime_info A\n            CROSS JOIN anime_info B\n        )\n    \n    ),\n    anime_anime AS (\n        \n        SELECT  A.animeA AS animeA, \n                A.animeB AS animeB, \n                COALESCE(A.co_occurence_cnt, 0) AS co_occurence_cnt, \n                COALESCE(B.related, 0) AS related, \n                COALESCE(C.num_recommenders, 0) AS num_recommenders,\n                COALESCE(D.cosine_sim, 0) AS cosine_sim,\n                COALESCE(A.bucket_co_occurence_cnt, 0) AS bucket_co_occurence_cnt, \n                COALESCE(C.bucket_num_recommenders, 0) AS bucket_num_recommenders,\n                COALESCE(D.bucket_cosine_sim, 0) AS bucket_cosine_sim\n        FROM anime_co_completed_anime A\n        FULL OUTER JOIN anime_related_anime B\n        ON A.animeA = B.animeA AND A.animeB = B.animeB\n        FULL OUTER JOIN anime_recommended_anime C\n        ON A.animeA = C.animeA AND A.animeB = C.animeB    \n        FULL OUTER JOIN anime_genre_sim_anime D\n        ON A.animeA = D.animeA AND A.animeB = D.animeB \n    \n    ),\n    anime_anime_ordered AS (\n        \n        SELECT *,\n            ROW_NUMBER() OVER (PARTITION BY animeA ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeB_rank,\n            ROW_NUMBER() OVER (PARTITION BY animeB ORDER BY related DESC, bucket_cosine_sim DESC, bucket_num_recommenders DESC, bucket_co_occurence_cnt DESC) AS animeA_rank\n        FROM anime_anime\n    \n    ),\n    easy_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 < B.animeB_rank AND A.animeB_rank + 600 > B.animeB_rank\n    ),\n    hard_positive_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 1 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank < B.animeB_rank AND A.animeB_rank + 100 > B.animeB_rank\n    ),\n    easy_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank + 500 > B.animeB_rank AND A.animeB_rank + 600 < B.animeB_rank\n    ),\n    hard_negative_pairs AS (\n        SELECT A.animeA AS anime_id, A.animeB AS retrieved_anime_id_1, B.animeB AS retrieved_anime_id_2, 0 AS label\n        FROM anime_anime_ordered A\n        LEFT JOIN anime_anime_ordered B\n        ON A.animeA = B.animeA\n        WHERE A.animeB_rank > B.animeB_rank AND A.animeB_rank + 100 < B.animeB_rank\n    ),\n    all_pairs AS (\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_positive_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM easy_negative_pairs\n        UNION DISTINCT\n        SELECT anime_id, retrieved_anime_id_1, retrieved_anime_id_2, label FROM hard_negative_pairs\n    )\n    \n        SELECT *\n        FROM all_pairs \n        WHERE ABS(MOD(FARM_FINGERPRINT(CONCAT(anime_id, retrieved_anime_id_1)), 10)) = 9 AND ABS(MOD(FARM_FINGERPRINT(retrieved_anime_id_2), 10)) = 0\n        "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "DATA: test anime anime ranking"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-8": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-8"
              },
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_user_last_watched"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n        WITH \n        list_anime AS (\n            \n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    \n        ),\n        filtered_user_anime_on_anime AS (\n            \n        SELECT A.*\n        FROM `anime-rec-dev.processed_area.user_anime` A\n        INNER JOIN list_anime B\n        ON A.anime_id = B.anime_id\n    \n        ),\n        user_anime AS (\n            \n        SELECT user_id, anime_id, score, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY last_interaction_date DESC) AS user_anime_order\n        FROM filtered_user_anime_on_anime\n        WHERE status = 'completed' AND last_interaction_date IS NOT NULL\n     AND score IS NOT NULL AND score > 5\n        )\n        SELECT user_id, anime_id FROM user_anime WHERE user_anime_order = 1\n    "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: user last watched"
              }
            },
            "run-query-save-to-bq-table-and-gcs-and-vertexai-9": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-9"
              },
              "dependentTasks": [
                "gcs-to-bq-table-and-vertexai-2",
                "run-query-save-to-bq-table-and-gcs-and-vertexai-8"
              ],
              "inputs": {
                "parameters": {
                  "destination_dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "destination_table_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "anime_anime_user_anime_ranked"
                      }
                    }
                  },
                  "gcs_output_format": {
                    "componentInputParameter": "pipelineparam--data_format"
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "\n        WITH \n        initial_recommendations AS (\n            SELECT A.user_id, B.retrieved_anime_id AS anime_id, B.score\n            FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}.anime_anime_user_last_watched` A\n            LEFT JOIN `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}.anime_anime_ranking_infer` B\n            ON A.anime_id = B.anime_id\n        )\n        \n        SELECT user_id, anime_id, MAX(score) AS score\n        FROM (\n            SELECT A.user_id,\n                   B.animeB AS anime_id,\n                   A.score,  \n                   ROW_NUMBER() OVER (PARTITION BY A.user_id, A.anime_id ORDER BY B.related_order ASC) AS new_related_order\n            FROM initial_recommendations A\n            LEFT JOIN `anime-rec-dev.processed_area.related_priority` B\n            ON A.anime_id = B.animeA\n            LEFT JOIN `anime-rec-dev.processed_area.user_anime` C\n            ON B.animeB = C.anime_id\n            WHERE C.status IS NULL OR C.status = 'plan_to_watch'\n        )\n        WHERE new_related_order = 1\n        GROUP BY user_id, anime_id\n    \n    "
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "INFER: user anime ranked"
              }
            }
          }
        },
        "inputDefinitions": {
          "artifacts": {
            "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "pipelineparam--data_format": {
              "type": "STRING"
            },
            "pipelineparam--dataset_id": {
              "type": "STRING"
            },
            "pipelineparam--project_id": {
              "type": "STRING"
            },
            "pipelineparam--run_retrieval": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "component-2-metrics_path": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "component-metrics_path": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "get-model-training-details-2-output_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "get-model-training-details-output_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-gcs-to-bq-table-and-vertexai": {
        "executorLabel": "exec-gcs-to-bq-table-and-vertexai",
        "inputDefinitions": {
          "artifacts": {
            "gcs_input_data": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_input_data_format": {
              "type": "STRING"
            },
            "gcs_input_data_schema": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-gcs-to-bq-table-and-vertexai-2": {
        "executorLabel": "exec-gcs-to-bq-table-and-vertexai-2",
        "inputDefinitions": {
          "artifacts": {
            "gcs_input_data": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_input_data_format": {
              "type": "STRING"
            },
            "gcs_input_data_schema": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-gcs-to-bq-table-and-vertexai-3": {
        "executorLabel": "exec-gcs-to-bq-table-and-vertexai-3",
        "inputDefinitions": {
          "artifacts": {
            "gcs_input_data": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_input_data_format": {
              "type": "STRING"
            },
            "gcs_input_data_schema": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-get-model-training-details": {
        "executorLabel": "exec-get-model-training-details",
        "inputDefinitions": {
          "artifacts": {
            "input_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "input_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "labels": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "output_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-get-model-training-details-2": {
        "executorLabel": "exec-get-model-training-details-2",
        "inputDefinitions": {
          "artifacts": {
            "input_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "input_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "labels": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "output_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-get-model-training-details-3": {
        "executorLabel": "exec-get-model-training-details-3",
        "inputDefinitions": {
          "artifacts": {
            "input_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "input_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "labels": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "output_model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-10": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-10",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-11": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-11",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-12": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-12",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-13": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-13",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-14": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-14",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-15": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-15",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-2": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-2",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-3": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-3",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-4": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-4",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-5": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-5",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-6": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-6",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-7": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-7",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-8": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-8",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-run-query-save-to-bq-table-and-gcs-and-vertexai-9": {
        "executorLabel": "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-9",
        "inputDefinitions": {
          "parameters": {
            "destination_dataset_id": {
              "type": "STRING"
            },
            "destination_table_id": {
              "type": "STRING"
            },
            "gcs_output_format": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_data_path": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-component": {
          "container": {
            "command": [
              "python3",
              "task.py",
              "--data-format",
              "{{$.inputs.parameters['data_format']}}",
              "--train-data-path",
              "{{$.inputs.artifacts['train_data_path'].path}}",
              "--val-data-path",
              "{{$.inputs.artifacts['val_data_path'].path}}",
              "--test-data-path",
              "{{$.inputs.artifacts['test_data_path'].path}}",
              "--anime-data-path",
              "{{$.inputs.artifacts['anime_data_path'].path}}",
              "--model-path",
              "{{$.outputs.artifacts['model_path'].path}}",
              "--metrics-path",
              "{{$.outputs.artifacts['metrics_path'].path}}",
              "--anime-embedding-size",
              "{{$.inputs.parameters['anime_embedding_size']}}",
              "--learning-rate",
              "{{$.inputs.parameters['learning_rate']}}",
              "--optimizer",
              "{{$.inputs.parameters['optimizer']}}",
              "--max-num-epochs",
              "{{$.inputs.parameters['max_num_epochs']}}",
              "--early-stop-num-epochs",
              "{{$.inputs.parameters['early_stop_num_epochs']}}"
            ],
            "image": "gcr.io/anime-rec-dev/anime_anime_train_retrieval:latest",
            "resources": {
              "cpuLimit": 16.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-component-2": {
          "container": {
            "command": [
              "python3",
              "task.py",
              "--data-format",
              "{{$.inputs.parameters['data_format']}}",
              "--train-data-path",
              "{{$.inputs.artifacts['train_data_path'].path}}",
              "--val-data-path",
              "{{$.inputs.artifacts['val_data_path'].path}}",
              "--test-data-path",
              "{{$.inputs.artifacts['test_data_path'].path}}",
              "--anime-data-path",
              "{{$.inputs.artifacts['anime_data_path'].path}}",
              "--model-path",
              "{{$.outputs.artifacts['model_path'].path}}",
              "--metrics-path",
              "{{$.outputs.artifacts['metrics_path'].path}}",
              "--anime-embedding-size",
              "{{$.inputs.parameters['anime_embedding_size']}}",
              "--scoring-layer-size",
              "{{$.inputs.parameters['scoring_layer_size']}}",
              "--learning-rate",
              "{{$.inputs.parameters['learning_rate']}}",
              "--optimizer",
              "{{$.inputs.parameters['optimizer']}}",
              "--max-num-epochs",
              "{{$.inputs.parameters['max_num_epochs']}}",
              "--early-stop-num-epochs",
              "{{$.inputs.parameters['early_stop_num_epochs']}}"
            ],
            "image": "gcr.io/anime-rec-dev/anime_anime_train_ranking:latest",
            "resources": {
              "cpuLimit": 16.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-component-3": {
          "container": {
            "command": [
              "python3",
              "task.py",
              "--data-format",
              "{{$.inputs.parameters['data_format']}}",
              "--model-path",
              "{{$.inputs.artifacts['model_path'].path}}",
              "--input-data-path",
              "{{$.inputs.artifacts['input_data_path'].path}}",
              "--output-data-path",
              "{{$.outputs.artifacts['output_data_path'].path}}"
            ],
            "image": "gcr.io/anime-rec-dev/anime_anime_infer_retrieval:latest",
            "resources": {
              "cpuLimit": 16.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-component-4": {
          "container": {
            "command": [
              "python3",
              "task.py",
              "--data-format",
              "{{$.inputs.parameters['data_format']}}",
              "--model-path",
              "{{$.inputs.artifacts['model_path'].path}}",
              "--input-data-path",
              "{{$.inputs.artifacts['input_data_path'].path}}",
              "--output-data-path",
              "{{$.outputs.artifacts['output_data_path'].path}}"
            ],
            "image": "gcr.io/anime-rec-dev/anime_anime_infer_ranking:latest",
            "resources": {
              "cpuLimit": 16.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-component-5": {
          "container": {
            "command": [
              "python3",
              "task.py",
              "--data-format",
              "{{$.inputs.parameters['data_format']}}",
              "--train-data-path",
              "{{$.inputs.artifacts['train_data_path'].path}}",
              "--val-data-path",
              "{{$.inputs.artifacts['val_data_path'].path}}",
              "--test-data-path",
              "{{$.inputs.artifacts['test_data_path'].path}}",
              "--anime-data-path",
              "{{$.inputs.artifacts['anime_data_path'].path}}",
              "--model-path",
              "{{$.outputs.artifacts['model_path'].path}}",
              "--metrics-path",
              "{{$.outputs.artifacts['metrics_path'].path}}",
              "--anime-embedding-size",
              "{{$.inputs.parameters['anime_embedding_size']}}",
              "--scoring-layer-size",
              "{{$.inputs.parameters['scoring_layer_size']}}",
              "--learning-rate",
              "{{$.inputs.parameters['learning_rate']}}",
              "--optimizer",
              "{{$.inputs.parameters['optimizer']}}",
              "--max-num-epochs",
              "{{$.inputs.parameters['max_num_epochs']}}",
              "--early-stop-num-epochs",
              "{{$.inputs.parameters['early_stop_num_epochs']}}"
            ],
            "image": "gcr.io/anime-rec-dev/anime_anime_train_ranking:latest",
            "resources": {
              "cpuLimit": 16.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-component-6": {
          "container": {
            "command": [
              "python3",
              "task.py",
              "--data-format",
              "{{$.inputs.parameters['data_format']}}",
              "--model-path",
              "{{$.inputs.artifacts['model_path'].path}}",
              "--input-data-path",
              "{{$.inputs.artifacts['input_data_path'].path}}",
              "--output-data-path",
              "{{$.outputs.artifacts['output_data_path'].path}}"
            ],
            "image": "gcr.io/anime-rec-dev/anime_anime_infer_ranking:latest",
            "resources": {
              "cpuLimit": 16.0,
              "memoryLimit": 32.0
            }
          }
        },
        "exec-gcs-to-bq-table-and-vertexai": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "gcs_to_bq_table_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef gcs_to_bq_table_and_vertexai(\n    gcs_input_data:Input[Dataset],\n    gcs_input_data_format: str,\n    gcs_input_data_schema:list,\n    project_id:str, \n    destination_dataset_id: str, \n    destination_table_id: str\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    if gcs_input_data_format == 'csv':\n        job_config = bigquery.LoadJobConfig(\n            schema=[bigquery.SchemaField(x[0], x[1]) for x in gcs_input_data_schema],\n            skip_leading_rows=1,\n            field_delimiter=',',\n            source_format=bigquery.SourceFormat.CSV\n        )\n    elif gcs_input_data_format == 'avro':\n        job_config = bigquery.LoadJobConfig(\n            schema=[bigquery.SchemaField(x[0], x[1]) for x in gcs_input_data_schema],\n            source_format=bigquery.SourceFormat.AVRO,\n            use_avro_logical_types=True\n        )\n    else:\n        raise(f\"{gcs_input_data_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_input_data = gcs_input_data.path.replace('/gcs/', 'gs://') + \"/*\"\n\n    load_job = client.load_table_from_uri(\n        source_uris = gcs_input_data, \n        destination = table_ref, \n        job_config = job_config\n    )\n    logging.info(f\"Started exporting {gcs_input_data} to BQ table {table_ref.path}\")\n    load_job.result()\n    logging.info(f\"Started exporting {gcs_input_data} to BQ table {table_ref.path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_input_data[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-gcs-to-bq-table-and-vertexai-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "gcs_to_bq_table_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef gcs_to_bq_table_and_vertexai(\n    gcs_input_data:Input[Dataset],\n    gcs_input_data_format: str,\n    gcs_input_data_schema:list,\n    project_id:str, \n    destination_dataset_id: str, \n    destination_table_id: str\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    if gcs_input_data_format == 'csv':\n        job_config = bigquery.LoadJobConfig(\n            schema=[bigquery.SchemaField(x[0], x[1]) for x in gcs_input_data_schema],\n            skip_leading_rows=1,\n            field_delimiter=',',\n            source_format=bigquery.SourceFormat.CSV\n        )\n    elif gcs_input_data_format == 'avro':\n        job_config = bigquery.LoadJobConfig(\n            schema=[bigquery.SchemaField(x[0], x[1]) for x in gcs_input_data_schema],\n            source_format=bigquery.SourceFormat.AVRO,\n            use_avro_logical_types=True\n        )\n    else:\n        raise(f\"{gcs_input_data_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_input_data = gcs_input_data.path.replace('/gcs/', 'gs://') + \"/*\"\n\n    load_job = client.load_table_from_uri(\n        source_uris = gcs_input_data, \n        destination = table_ref, \n        job_config = job_config\n    )\n    logging.info(f\"Started exporting {gcs_input_data} to BQ table {table_ref.path}\")\n    load_job.result()\n    logging.info(f\"Started exporting {gcs_input_data} to BQ table {table_ref.path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_input_data[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-gcs-to-bq-table-and-vertexai-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "gcs_to_bq_table_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef gcs_to_bq_table_and_vertexai(\n    gcs_input_data:Input[Dataset],\n    gcs_input_data_format: str,\n    gcs_input_data_schema:list,\n    project_id:str, \n    destination_dataset_id: str, \n    destination_table_id: str\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    if gcs_input_data_format == 'csv':\n        job_config = bigquery.LoadJobConfig(\n            schema=[bigquery.SchemaField(x[0], x[1]) for x in gcs_input_data_schema],\n            skip_leading_rows=1,\n            field_delimiter=',',\n            source_format=bigquery.SourceFormat.CSV\n        )\n    elif gcs_input_data_format == 'avro':\n        job_config = bigquery.LoadJobConfig(\n            schema=[bigquery.SchemaField(x[0], x[1]) for x in gcs_input_data_schema],\n            source_format=bigquery.SourceFormat.AVRO,\n            use_avro_logical_types=True\n        )\n    else:\n        raise(f\"{gcs_input_data_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_input_data = gcs_input_data.path.replace('/gcs/', 'gs://') + \"/*\"\n\n    load_job = client.load_table_from_uri(\n        source_uris = gcs_input_data, \n        destination = table_ref, \n        job_config = job_config\n    )\n    logging.info(f\"Started exporting {gcs_input_data} to BQ table {table_ref.path}\")\n    load_job.result()\n    logging.info(f\"Started exporting {gcs_input_data} to BQ table {table_ref.path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_input_data[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-get-model-training-details": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_model_training_details"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas==1.3.4' 'fsspec==2022.2.0' 'gcsfs==2022.2.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_model_training_details(\n    project_id:str,\n    input_model: Input[Model],\n    model_name:str,\n    labels:dict,\n    input_metrics: Input[Metrics],\n    output_model: Output[Model],\n    output_metrics: Output[Metrics]\n):  \n    import pandas as pd\n    from google.cloud import aiplatform\n\n    output_model.metadata[\"model_name\"] = model_name\n    for k, v in labels.items():\n        output_model.metadata[k] = v\n    output_model.path = f\"{input_model.path}/\"\n\n    metrics_df = pd.read_json(f\"{input_metrics.path}/metrics.json\")\n    for metric in metrics_df.iterrows():\n        metric_name = metric[1]['metrics']['name']\n        metric_value = metric[1]['metrics']['number_value']\n        output_metrics.log_metric(metric_name, metric_value)\n        output_model.metadata[metric_name] = metric_value\n\n    labels = {k: str(v).replace(\".\", \"\") for k, v in labels.items()}\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n    aiplatform.Model.upload(\n        display_name = model_name,\n        artifact_uri = input_model.path,\n        serving_container_image_uri=\"gcr.io/anime-rec-dev/user_anime_infer_ranking:latest\",\n        labels = labels\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-get-model-training-details-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_model_training_details"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas==1.3.4' 'fsspec==2022.2.0' 'gcsfs==2022.2.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_model_training_details(\n    project_id:str,\n    input_model: Input[Model],\n    model_name:str,\n    labels:dict,\n    input_metrics: Input[Metrics],\n    output_model: Output[Model],\n    output_metrics: Output[Metrics]\n):  \n    import pandas as pd\n    from google.cloud import aiplatform\n\n    output_model.metadata[\"model_name\"] = model_name\n    for k, v in labels.items():\n        output_model.metadata[k] = v\n    output_model.path = f\"{input_model.path}/\"\n\n    metrics_df = pd.read_json(f\"{input_metrics.path}/metrics.json\")\n    for metric in metrics_df.iterrows():\n        metric_name = metric[1]['metrics']['name']\n        metric_value = metric[1]['metrics']['number_value']\n        output_metrics.log_metric(metric_name, metric_value)\n        output_model.metadata[metric_name] = metric_value\n\n    labels = {k: str(v).replace(\".\", \"\") for k, v in labels.items()}\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n    aiplatform.Model.upload(\n        display_name = model_name,\n        artifact_uri = input_model.path,\n        serving_container_image_uri=\"gcr.io/anime-rec-dev/user_anime_infer_ranking:latest\",\n        labels = labels\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-get-model-training-details-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_model_training_details"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas==1.3.4' 'fsspec==2022.2.0' 'gcsfs==2022.2.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_model_training_details(\n    project_id:str,\n    input_model: Input[Model],\n    model_name:str,\n    labels:dict,\n    input_metrics: Input[Metrics],\n    output_model: Output[Model],\n    output_metrics: Output[Metrics]\n):  \n    import pandas as pd\n    from google.cloud import aiplatform\n\n    output_model.metadata[\"model_name\"] = model_name\n    for k, v in labels.items():\n        output_model.metadata[k] = v\n    output_model.path = f\"{input_model.path}/\"\n\n    metrics_df = pd.read_json(f\"{input_metrics.path}/metrics.json\")\n    for metric in metrics_df.iterrows():\n        metric_name = metric[1]['metrics']['name']\n        metric_value = metric[1]['metrics']['number_value']\n        output_metrics.log_metric(metric_name, metric_value)\n        output_model.metadata[metric_name] = metric_value\n\n    labels = {k: str(v).replace(\".\", \"\") for k, v in labels.items()}\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n    aiplatform.Model.upload(\n        display_name = model_name,\n        artifact_uri = input_model.path,\n        serving_container_image_uri=\"gcr.io/anime-rec-dev/user_anime_infer_ranking:latest\",\n        labels = labels\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-10": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-11": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-12": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-13": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-14": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-15": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-4": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-5": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-6": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-7": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-8": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-run-query-save-to-bq-table-and-gcs-and-vertexai-9": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "run_query_save_to_bq_table_and_gcs_and_vertexai"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.31.0' 'google-cloud-aiplatform==1.11.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef run_query_save_to_bq_table_and_gcs_and_vertexai(\n    query: str,\n    project_id: str, \n    destination_dataset_id: str, \n    destination_table_id: str,\n    gcs_output_format: str,\n    output_data_path: Output[Dataset]\n):\n\n    import logging\n    from google.cloud import bigquery\n    from google.api_core.exceptions import Conflict \n\n    logging.basicConfig(\n        format='%(levelname)s: %(asctime)s: %(message)s',\n        level=logging.INFO\n    )\n\n    client = bigquery.Client(project=project_id)\n\n    try:\n        dataset = bigquery.Dataset(f\"{project_id}.{destination_dataset_id}\")\n        dataset.location = \"us-central1\"\n        dataset = client.create_dataset(dataset, timeout=30)\n        logging.info(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n    except Conflict as e:\n        pass\n\n    destination_table_ref = client.dataset(destination_dataset_id).table(destination_table_id)\n\n    job_config = bigquery.QueryJobConfig()\n    job_config.destination = destination_table_ref\n\n    query_job = client.query(\n        query=query,\n        location='us-central1',\n        job_config=job_config\n    )\n    logging.info(f\"Started running query and saving results to BQ table : {destination_table_ref.path}\")\n\n    query_job.result()\n    logging.info(f\"Finished running query and saved results to BQ table : {destination_table_ref.path}\")\n    logging.info(f\"{destination_table_ref.path} has {client.get_table(destination_table_ref).num_rows} rows\")\n\n    if gcs_output_format == 'csv':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.field_delimiter = ','\n        job_config.destination_format = bigquery.job.DestinationFormat.CSV\n\n    elif gcs_output_format == 'avro':\n        job_config = bigquery.job.ExtractJobConfig()\n        job_config.destination_format = bigquery.job.DestinationFormat.AVRO\n        job_config.use_avro_logical_types = True\n        job_config.compression = bigquery.Compression.SNAPPY\n\n    else:\n        raise(f\"{gcs_output_format} format is not supported. Specify either 'CSV' or 'AVRO'\")\n\n    gcs_output_path = output_data_path.path.replace('/gcs/', 'gs://') + \"/*\"\n    extract_job = client.extract_table(\n        source=destination_table_ref,\n        destination_uris=gcs_output_path,\n        location=\"us-central1\",\n        job_config=job_config\n    )\n    logging.info(f\"Started exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    extract_job.result()\n    logging.info(f\"Finished exporting BQ table {destination_table_ref.path} to {gcs_output_path}\")\n\n    client.close()\n\n    from google.cloud import aiplatform\n    dataset_name = \"_\".join(destination_dataset_id.split(\"_\")[:-1])\n    dataset_name = f\"{dataset_name}_{destination_table_id}\"\n\n    aiplatform.init(\n        project=project_id, \n        location='us-central1',\n        staging_bucket='gs://anime-rec-dev-ml-pipelines'\n    )\n\n    _ = aiplatform.TabularDataset.create(\n        display_name=dataset_name, \n        gcs_source=[gcs_output_path[:-1]]\n    )\n\n"
            ],
            "image": "python:3.7"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "anime-anime-recommendation-pipeline"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "component-2-metrics_path": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "component-2-metrics_path",
                  "producerSubtask": "condition-yes-run-retrieval-1"
                }
              ]
            },
            "component-5-metrics_path": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "component-5-metrics_path",
                  "producerSubtask": "condition-no-run-retrieval-2"
                }
              ]
            },
            "component-metrics_path": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "component-metrics_path",
                  "producerSubtask": "condition-yes-run-retrieval-1"
                }
              ]
            },
            "get-model-training-details-2-output_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "get-model-training-details-2-output_metrics",
                  "producerSubtask": "condition-yes-run-retrieval-1"
                }
              ]
            },
            "get-model-training-details-3-output_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "get-model-training-details-3-output_metrics",
                  "producerSubtask": "condition-no-run-retrieval-2"
                }
              ]
            },
            "get-model-training-details-output_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "get-model-training-details-output_metrics",
                  "producerSubtask": "condition-yes-run-retrieval-1"
                }
              ]
            }
          }
        },
        "tasks": {
          "condition-no-run-retrieval-2": {
            "componentRef": {
              "name": "comp-condition-no-run-retrieval-2"
            },
            "dependentTasks": [
              "run-query-save-to-bq-table-and-gcs-and-vertexai"
            ],
            "inputs": {
              "artifacts": {
                "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_data_path",
                    "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai"
                  }
                }
              },
              "parameters": {
                "pipelineparam--data_format": {
                  "componentInputParameter": "data_format"
                },
                "pipelineparam--dataset_id": {
                  "componentInputParameter": "dataset_id"
                },
                "pipelineparam--project_id": {
                  "componentInputParameter": "project_id"
                },
                "pipelineparam--run_retrieval": {
                  "componentInputParameter": "run_retrieval"
                }
              }
            },
            "taskInfo": {
              "name": "condition-no-run-retrieval-2"
            },
            "triggerPolicy": {
              "condition": "inputs.parameters['pipelineparam--run_retrieval'].string_value == 'false'"
            }
          },
          "condition-yes-run-retrieval-1": {
            "componentRef": {
              "name": "comp-condition-yes-run-retrieval-1"
            },
            "dependentTasks": [
              "run-query-save-to-bq-table-and-gcs-and-vertexai"
            ],
            "inputs": {
              "artifacts": {
                "pipelineparam--run-query-save-to-bq-table-and-gcs-and-vertexai-output_data_path": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_data_path",
                    "producerTask": "run-query-save-to-bq-table-and-gcs-and-vertexai"
                  }
                }
              },
              "parameters": {
                "pipelineparam--data_format": {
                  "componentInputParameter": "data_format"
                },
                "pipelineparam--dataset_id": {
                  "componentInputParameter": "dataset_id"
                },
                "pipelineparam--project_id": {
                  "componentInputParameter": "project_id"
                },
                "pipelineparam--run_retrieval": {
                  "componentInputParameter": "run_retrieval"
                }
              }
            },
            "taskInfo": {
              "name": "condition-yes-run-retrieval-1"
            },
            "triggerPolicy": {
              "condition": "inputs.parameters['pipelineparam--run_retrieval'].string_value == 'true'"
            }
          },
          "run-query-save-to-bq-table-and-gcs-and-vertexai": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-run-query-save-to-bq-table-and-gcs-and-vertexai"
            },
            "inputs": {
              "parameters": {
                "destination_dataset_id": {
                  "componentInputParameter": "dataset_id"
                },
                "destination_table_id": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "list_anime"
                    }
                  }
                },
                "gcs_output_format": {
                  "componentInputParameter": "data_format"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                },
                "query": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "\n        SELECT anime_id\n        FROM `anime-rec-dev.processed_area.user_anime`\n        WHERE status = 'completed' AND score IS NOT NULL\n        GROUP BY anime_id\n        HAVING COUNT(*) >= 1000\n    "
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "DATA: list anime"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "data_format": {
            "type": "STRING"
          },
          "dataset_id": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "run_retrieval": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "component-2-metrics_path": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "component-5-metrics_path": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "component-metrics_path": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "get-model-training-details-2-output_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "get-model-training-details-3-output_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "get-model-training-details-output_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.10"
  },
  "runtimeConfig": {
    "parameters": {
      "data_format": {
        "stringValue": "csv"
      },
      "dataset_id": {
        "stringValue": "ml_pipelines"
      },
      "project_id": {
        "stringValue": "anime-rec-dev"
      },
      "run_retrieval": {
        "stringValue": "false"
      }
    }
  }
}